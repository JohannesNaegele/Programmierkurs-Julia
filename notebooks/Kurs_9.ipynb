{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Kurs 9"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Zufallszahlen"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Für viele Anwendungen braucht man Zufallszahlen. Zwar haben wir in Julia nur Pseudozufallszahlen (die Zahlen werden nach einer deterministischen Methode berechnet), die sich zudem irgendwann wiederholen (das nennt man Periode). Allerdings verhalten sich die Zahlen im besten Fall sehr ähnlich zu tatsächlich zufällig (aus einer Verteilung) gezogenen Zahlen und die Periode ist oft so groß, dass sie in der Praxis keine Rolle spielt (für den [Mersenne-Twister](https://de.wikipedia.org/wiki/Mersenne-Twister): `4.3*10^6001`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Pkg\n",
    "Pkg.add(\"Random\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Random"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Zentral sind die ab Werk verfügbaren Sampler der uniformen Verteilung und Normalverteilung:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand() # uniform verteilter Wert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "randn() # normalverteilter Wert"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um zu zeigen, dass die Funktionen funktionieren wie gedacht, ist hier ein Histogramm mit Kerndichteschätzer (wenn einem das nichts sagt – einfach ignorieren)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"KernelDensity\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using Plots\n",
    "using KernelDensity\n",
    "\n",
    "data = randn(100000)\n",
    "density = kde(data)\n",
    "histogram(\n",
    "    data,\n",
    "    normalize = true\n",
    ")\n",
    "plot!(density.x, density.density, linewidth=2, label=\"KDE\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Um reproduzierbare Zufallszahlen zu generieren, setzen wir einen sogenannten *seed*. Das ist extrem nützlich, wenn wir zum Beispiel Simulationen in einer Studie überprüfbar machen wollen."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "println(rand(2))\n",
    "println(rand(2))\n",
    "\n",
    "Random.seed!(1) # setze den seed auf 1\n",
    "println(rand(2))\n",
    "println(rand(2))\n",
    "\n",
    "Random.seed!(1) # setze den seed auf 1\n",
    "println(rand(2))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Distributions.jl\n",
    "\n",
    "Das Paket Distributions.jl stellt uns viele weitere Verteilungen bereit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"Distributions\")\n",
    "using Distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand(Cauchy())"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance\n",
    "\n",
    "Julia ist per se ziemlich schnell – das ist ja gerade einer der Gründe, warum wir diese Sprache nutzen. Allerdings gibt es für ein bestimmtes Problem oft viele Wege nach Rom, wobei manche effizienter als andere sind. In manchen Fällen ist es sogar so, dass die Suche nach Optimierungen ein Fass ohne Boden ist.\n",
    "\n",
    "Gerade deshalb sollte man sich aber nicht immer den Kopf über jedes Detail zerbrechen, das möglicherweise performancerelevant ist. Donald E. Knuth (legendärer Programmierer und unter anderem Erfinder von TeX) sagte dazu mal: „Premature optimization is the root of all evil“. In anderen Worten: Meistens sollte man eher versuchen schönen generischen Code zu schreiben und sich hinterher um Details kümmern, als sich umgekehrt in diesen zu verrennen – was nicht nur Zeit kostet, sondern im schlimmsten Fall zu Spaghetticode führt (siehe auch [hier](https://wiki.c2.com/?PrematureOptimization)).\n",
    "\n",
    "Sinnvoll ist dagegen zum Beispiel ein sogenannter Profiler (etwa `@profview`), der uns Fingerzeige liefert, wo möglicherweise Speed flöten geht."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Type instabilities\n",
    "\n",
    "Julia ist deshalb schnell, weil es für unterschiedliche Typen als Inputs auch tatsächlich verschiedene Funktionen (oder Codeabschnitte) in Assembler kompiliert. Man muss also nicht innerhalb der Funktion sozusagen wieder abchecken: \"Was wäre, wenn hier jetzt diese Operation mit Typ X wäre?\". Stattdessen wird die Funktion von vornherein spezialisiert. Umgekehrt müssen wir diesen Prozess nicht händisch wie in C/C++ machen, stattdessen wird das für uns von Julia übernommen.\n",
    "\n",
    "Es gibt allerdings Fälle, in denen ist Julia sich nicht sicher, ob sich der Typ innerhalb eines Codeabschnitts ändern kann und geht deshalb auf Nummer sicher. Soll heißen: Dieser Codeabschnitt ist dann weniger stark spezialisert und operiert üblicherweise auf Typen der Art ```Union{Typ_1, Typ_2}```. Das ist auch einer der Gründe, warum man *globale Variablen* vermeiden sollte (wenngleich sich hier nicht nur der Typ, sondern zusätzlich auch der Wert unvorhergesehen ändern kann). Für die, die es genauer wissen wollen, [hier](https://discourse.julialang.org/t/why-type-instability/4013/19) ein kleiner Thread zur Frage, warum es überhaupt type instabilities gibt."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realitätsnahes Beispiel 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"BenchmarkTools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "```@time```, printed die benötigte Zeit und gibt das Ergebnis zurück. Aber Achtung: Beim ersten Ausführen muss der Code erst kompiliert, sodass wir beim ersten Mal immer langsamer sind!. Deshalb benutzen wir das Makro ```@btime```, das mehrere Durchläufe macht und die Zeit zum Kompilieren ignoriert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aa = Any[1:1000;];\n",
    "ai = [1:1000;];\n",
    "\n",
    "@btime sum($aa)\n",
    "@btime sum($ai)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_warntype sum(aa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@code_warntype sum(ai)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Realitätsnahes Beispiel 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct RealPoint\n",
    "    x::Real\n",
    "    y::Real\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hier haben wir den abstrakten Typ `Real` als Typ unserer Felder verwendet. Das funktioniert zwar, aber ist nicht besonders toll. Denn `Real` ist kein konkreter Typ: Der Julia-Compiler kann keine Annahmen über das Datenlayout von RealPoint treffen, und die Methodenauswahl muss zur Laufzeit und nicht zur Kompilierzeit erfolgen. Der richtige Weg, dies zu tun, wäre wie folgt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "struct Point{T<:Real}\n",
    "    x::T\n",
    "    y::T\n",
    "end"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Row vs. column major\n",
    "\n",
    "#### Konzept\n",
    "\n",
    "In den meisten Programmiersprachen sind Matrizen bzw. Arrays technisch nichts anderes als eindimensionale Arrays, also quasi Vektoren (Achtung: Wir reden hier nicht von echten Typen der Art ```Vector```). Das bedeutet: Elemente werden zeilen- oder spaltenweise (row-/column major) sukzessive in einer langen Liste gespeichert.\n",
    "\n",
    "![Alt text](../graphics/rowcolumnarrays.png)\n",
    "\n",
    "Es kommt ein bisschen auf Hardware bzw. konkrete Architektur an, aber generell gilt: Man möchte tendenziell auf (physisch) nahe beieinanderliegende Elemente zugreifen, weil der Zugriff dann schneller erfolgt. In anderen Worten: Im Algorithmus sollten häufige Sprünge möglichst vermieden werden!\n",
    "\n",
    "Ein Beispiel dafür wäre die Implementierung einer Matrixmultiplikation."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Beispiel: Matrixmultiplikation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"BenchmarkTools\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using BenchmarkTools\n",
    "\n",
    "m = 1000; n = 1000; k = 1000\n",
    "X = rand(m, k)\n",
    "Y = rand(k, n)\n",
    "Z = zeros(m, n)\n",
    "\n",
    "function naive_matmul!(A, B, C)\n",
    "    C .= 0\n",
    "    for i in 1:size(A)[1]\n",
    "        for j in 1:size(B)[2]\n",
    "            for k in 1:size(A)[2]\n",
    "                @inbounds C[i, j] += A[i, k] * B[k, j]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "@btime naive_matmul!(X, Y, Z)\n",
    "isapprox(Z, X * Y, atol = 1e-10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# lediglich Reihenfolge j, k, i ist anders\n",
    "function smart_matmul!(A, B, C)\n",
    "    C .= 0\n",
    "    # @simd bringt hier scheinbar nichts\n",
    "    @simd for j in 1:size(B)[2]\n",
    "        for k in 1:size(A)[2]\n",
    "            for i in 1:size(A)[1]\n",
    "                @inbounds C[i, j] += A[i, k] * B[k, j]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "@btime smart_matmul!(X, Y, Z)\n",
    "isapprox(Z, X * Y, atol = 1e-10)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hieran kann man auch gut erkennen, dass man in der Regel keine neuen Objekte erzeugen, sondern auf bereits existierende operieren möchte. Konkret: Die Matrix C wird nicht neu definiert und mit ```return``` zurückgegeben. Denn wäre dies der Fall, so müsste bei jedem Aufruf der Matrixmultiplikation neuer Speicher angelegt werden. Und wenn wir das öfters tun, kostet es Zeit."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Ausblick\n",
    "\n",
    "Viele Probleme, die aus mathematischer Sicht trivial sind, sind in der Implementierung alles andere als das. Wie man hier etwa sehen kann, sind wir immer noch mindestens Faktor 10 von einer wirklich performanten Version entfernt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime Z = X * Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ein großer Teil dieser Optimierung AVX."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Pkg.add(\"LoopVectorization\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# exportiert @turbo für AVX\n",
    "using LoopVectorization\n",
    "\n",
    "@inline function avx_matmul!(A, B, C)\n",
    "    C .= 0\n",
    "    @turbo for j in 1:size(B)[2]\n",
    "        for k in 1:size(A)[2]\n",
    "            for i in 1:size(A)[1]\n",
    "                @inbounds C[i, j] += A[i, k] * B[k, j]\n",
    "            end\n",
    "        end\n",
    "    end\n",
    "end\n",
    "\n",
    "@btime avx_matmul!(X, Y, Z)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Der Rest ist hauptsächlich \n",
    "- memory modelling\n",
    "- Cache/blocking inlining\n",
    "- packing, padding\n",
    "und stark von der Hardware abhängig.\n",
    "\n",
    "Natürlich ist das extrem abhängig von Matrixgröße und LoopVectorization besser, wenn caching keine große Rolle spielt. Siehe auch [hier](https://discourse.julialang.org/t/ann-loopvectorization/32843) und [Docs](https://juliasimd.github.io/LoopVectorization.jl/latest/examples/matrix_multiplication/). \n",
    "\n",
    "Hier beispielsweise ein interessanter [Thread]((https://discourse.julialang.org/t/julia-matrix-multiplication-performance/55175/18)), wie im Paket Octavian.jl Matrixmultiplikation optimiert wird."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kleinere Dimensionen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "using LinearAlgebra\n",
    "\n",
    "BLAS.get_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "function smarter_avx_matmul!(A, B, C)\n",
    "    # die Umordnung des Loops wird automatisch gemacht!\n",
    "    @turbo for i ∈ 1:size(A,1), j ∈ 1:size(B,2)\n",
    "        # so ist es sogar noch ein bisschen schneller, vermutlich weil dieses Statement besser parallelisiert werden kann\n",
    "        C[i,j] = 0.0\n",
    "        for k ∈ 1:size(A,2)\n",
    "            C[i,j] += A[i,k] * B[k,j]\n",
    "        end\n",
    "    end\n",
    "end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 100; n = 100; k = 100\n",
    "X = rand(m, k)\n",
    "Y = rand(k, n)\n",
    "Z = zeros(m, n)\n",
    "\n",
    "@btime smarter_avx_matmul!(X, Y, Z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime Z = X * Y"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Weiter Tipps findet man in der [Dokumentation](https://docs.julialang.org/en/v1/manual/performance-tips/)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kontextabhängige Optimierung\n",
    "\n",
    "Eine Sache, die man nicht vergessen darf, ist: Code ist oftmals nur für eine konkrete, spezielle Anwendung optimal. Kommen wir beispielsweise zu unseren Zufallszahlen von vorhin zurück. Da ist die Effizienz zum Teil abhängig davon, ob wir *oft* aus derselben Verteilung samplen, weil die Initialisierung des Samplers aufwendig sein kann."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sample from categorical distribution\n",
    "function categorical(probabilities)\n",
    "    u = rand()\n",
    "    sum = 0.0\n",
    "    for i in eachindex(probabilities)\n",
    "        sum += probabilities[i]\n",
    "        if u <= sum\n",
    "            return i\n",
    "        end\n",
    "    end\n",
    "end\n",
    "@btime categorical([0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.9])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unser naiver Code ist langsamer als der Sampler aus Distributions.jl:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist_often = Categorical([0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.9])\n",
    "@btime rand(dist_often)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dieser hat aber eine ziemlich schlechte Performance, wenn wir ihn in jeder Iteration neu definieren müssen:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@btime begin \n",
    "    dist_often = Categorical([0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.01, 0.9])\n",
    "    rand(dist_often)\n",
    "end"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Julia 1.10.0",
   "language": "julia",
   "name": "julia-1.10"
  },
  "language_info": {
   "file_extension": ".jl",
   "mimetype": "application/julia",
   "name": "julia",
   "version": "1.10.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
